{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website scraping\n",
    "\n",
    "---\n",
    "\n",
    "Group name: C\n",
    "\n",
    "---\n",
    "\n",
    "Scrape an article from one of the following websites (you can choose one of them):\n",
    "\n",
    "-   [fivethirtyeight](https://fivethirtyeight.com/)\n",
    "-   [cnn](https://edition.cnn.com/) or\n",
    "-   [wired](https://www.wired.com/)\n",
    "\n",
    "In your notebook:\n",
    "\n",
    "-   Create a Pandas dataframe\n",
    "-   Save the data as CSV in the data subfolder `data/raw` as `webscraping.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "# Importieren der benötigten Bibliotheken\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOUP\n",
    "\n",
    "# URL des Artikels festlegen, via Requests anfragen und mit BeautifulSoup parsen\n",
    "url = 'https://edition.cnn.com/2022/12/12/us/common-questions-nuclear-fusion-climate/index.html'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE\n",
    "\n",
    "# Absätze identifizieren\n",
    "text = soup.find_all(\"p\", class_=\"inline-placeholder\")\n",
    "\n",
    "# Erstellen einer Liste für die Absätze\n",
    "paragraphs = []\n",
    "\n",
    "# Iteratives hinzufügen der Absätze via for-Schleife\n",
    "for i in text:\n",
    "  paragraphs.append(i.string)\n",
    "\n",
    "# Erstellen einer Pandas Series mit dem Typ String\n",
    "text = pd.Series(paragraphs, dtype=\"string\")\n",
    "\n",
    "# Entfernung von \\n\n",
    "text = text.str.replace('\\n', '')\n",
    "\n",
    "df = pd.DataFrame({\"text\": text})\n",
    "\n",
    "# Entfernung der leeren Zeilen\n",
    "df = df.dropna()\n",
    "\n",
    "# Alle Texte zusammenfügen\n",
    "all_text = ', '.join(df.text)\n",
    "\n",
    "# Artikeltext via Verkettung, Trennzeichen: Leer\n",
    "artikeltext = df[\"text\"].str.cat(sep=\" \")\n",
    "\n",
    "# Doppelte Leerzeichen durch einfache ersetzen\n",
    "while \"  \" in artikeltext:\n",
    "  artikeltext = artikeltext.replace(\"  \", \" \")\n",
    "\n",
    "# Nicht notwendige Leerzeichen entfernen\n",
    "artikeltext = artikeltext.strip()\n",
    "\n",
    "# Seitentitel\n",
    "n = soup.title.string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE\n",
    "\n",
    "# Erstellen des Dataframes\n",
    "index=[\"Titel\", \"Artikeltext\"]\n",
    "\n",
    "df1 = pd.DataFrame([n, artikeltext], index=index)\n",
    "\n",
    "# Transponieren des Dataframes\n",
    "df1 = df1.T\n",
    "\n",
    "# Abspeichern des Dataframes\n",
    "df1.to_csv(\"webscraping.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e15ac9eba63a5c09141544704f57ff910d35b346b5f59427653c91541004419a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
